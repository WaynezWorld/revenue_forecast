{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086a6fde-bdb3-46c7-8a32-f73e3a3aba04",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Dependency 'keyring' is not installed, cannot cache id token. You might experience multiple authentication pop ups while using ExternalBrowser/OAuth/MFA Authenticator. To avoid this please install keyring module using the following command:\n",
      " pip install snowflake-connector-python[secure-local-storage]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/a7ecaa8d-4880-4bcd-8c42-7d53d21b35bb/saml2?SAMLRequest=jZJRb9owFIX%2FSuQ9J3YSAqkFVAzUlanrGNBK3ZtjG7Bw7NR2krJfPyeA1D202lvknOPv%2BJ47vn0rZdBwY4VWExBHCARcUc2E2k%2FA0%2FYuzEFgHVGMSK34BJy4BbfTsSWlrPCsdge15q81ty7wFymLux8TUBuFNbHCYkVKbrGjeDP78YCTCGFiLTfO48DFwqzwrINzFYawbduoTSNt9jBBCEF0A72qk3wB7xDV54zKaKepllfLm3%2FTB4gYokGH8ApPWF2MX4U6j%2BAzSnEWWXy%2F3a7C1c%2FNFgSz6%2BvmWtm65GbDTSMof1o%2FnANYn4BqRUJKqWAmskq3O0mOnOqyqp2%2FL%2FJfcMcZlHov%2FJSWiwmojoJ9u1%2B%2BvMSvNT00zfG0zoY3a2dPMzVPn3mT6NHm8U9BeJktvv%2BiIHi%2Bdpp0nS6trflSdU06f4SSYYjiMEXbeIhRjLM8SrPsNwgWvkmhiOud17h9jqgU1Gird04rKRTvU5IRp4TkLBzkOQoHBWVhTgdJOGJZypK4SLOigF1fCTjvDO6DmOn%2FT2IM3%2Fsui%2Ffou1guVloKegrutCmJ%2B7iqOIr7E8HCXS%2FFvCRCzhgz3FpfmZS6nRtOnN9vZ2oO4PRM%2FXfDp38B&RelayState=ver%3A1-hint%3A268336721437094-ETMsDgAAAZwPo8pTABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEEg%2BOSGIOKhNUmbxQEpS218AAACQhI9WDYmHnqrcl7hDSfY5w9l3WHQ4pSqXnonfvWYz27T3PsktNXNYAHtTeAOjmM5ZFA9r1yVFyyza92SgiNXrZw8RxikLyKbWsni%2FI13ggo2fg8TIRxGCbnaprwKqGS0WPt1qDlKdX0KJ0usaZljiEZ8hnwT2J3RyYcgRzC%2Bx6G1TnyQkFVMHqxD2uMrWGDjGABTtnvYx1D%2BqsncaZ4%2FlPCP0tczNzg%3D%3D to authenticate...\n",
      "A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings.\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "250001 (08001): Failed to connect to DB: cona-ccci.snowflakecomputing.com:443. The user you were trying to authenticate as differs from the user currently logged in at the IDP.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSnowparkSessionException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowpark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_active_session\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m session = \u001b[43mget_active_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing active Snowflake session\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\snowpark\\context.py:115\u001b[39m, in \u001b[36mget_active_session\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns the current active Snowpark session.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03mRaises: SnowparkSessionException: If there is more than one active session or no active sessions.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    113\u001b[39m \u001b[33;03m    A :class:`Session` object for the current session.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msnowflake\u001b[49m\u001b[43m.\u001b[49m\u001b[43msnowpark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_active_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\snowpark\\session.py:332\u001b[39m, in \u001b[36m_get_active_session\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_NO_DEFAULT_SESSION()\n",
      "\u001b[31mSnowparkSessionException\u001b[39m: (1403): No default Session is found. Please create a session before you call function 'udf' or use decorator '@udf'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing active Snowflake session\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     12\u001b[39m     session = \u001b[43mSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcona-ccci\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwayne.jones@coca-cola.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauthenticator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexternalbrowser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBI_P_ADMIN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwarehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBI_P_QRY_PU_WH\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatabase\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDB_BI_P_SANDBOX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mschema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSANDBOX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated new Snowflake session with external browser auth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\snowpark\\session.py:524\u001b[39m, in \u001b[36mSession.SessionBuilder.create\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    522\u001b[39m     _add_session(session)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     session = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconnection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._app_name:\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_json:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\snowpark\\session.py:566\u001b[39m, in \u001b[36mSession.SessionBuilder._create_internal\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mparamstyle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options:\n\u001b[32m    564\u001b[39m     \u001b[38;5;28mself\u001b[39m._options[\u001b[33m\"\u001b[39m\u001b[33mparamstyle\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mqmark\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m new_session = Session(\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     ServerConnection({}, conn) \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mServerConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._options,\n\u001b[32m    568\u001b[39m )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options:\n\u001b[32m    571\u001b[39m     \u001b[38;5;28mself\u001b[39m._options[\u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:169\u001b[39m, in \u001b[36mServerConnection.__init__\u001b[39m\u001b[34m(self, options, conn)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mself\u001b[39m._lower_case_parameters = {k.lower(): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m options.items()}\n\u001b[32m    168\u001b[39m \u001b[38;5;28mself\u001b[39m._add_application_parameters()\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28mself\u001b[39m._conn = conn \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lower_case_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.max_string_size = DEFAULT_STRING_SIZE\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conn._session_parameters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\__init__.py:76\u001b[39m, in \u001b[36mConnect\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(SnowflakeConnection.\u001b[34m__init__\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mConnect\u001b[39m(**kwargs) -> SnowflakeConnection:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSnowflakeConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\connection.py:679\u001b[39m, in \u001b[36mSnowflakeConnection.__init__\u001b[39m\u001b[34m(self, connection_name, connections_file_path, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m     kwargs = _get_default_connection_params()\n\u001b[32m    678\u001b[39m \u001b[38;5;28mself\u001b[39m.__set_error_attributes()\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[38;5;28mself\u001b[39m._telemetry = TelemetryClient(\u001b[38;5;28mself\u001b[39m._rest)\n\u001b[32m    681\u001b[39m \u001b[38;5;28mself\u001b[39m.expired = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\connection.py:1174\u001b[39m, in \u001b[36mSnowflakeConnection.connect\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(exceptions_dict))\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;66;03m# Register the connection in the pool after successful connection\u001b[39;00m\n\u001b[32m   1177\u001b[39m _connections_registry.add_connection(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\connection.py:1602\u001b[39m, in \u001b[36mSnowflakeConnection.__open_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1595\u001b[39m     \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28mself\u001b[39m.auth_class = AuthByOkta(\n\u001b[32m   1597\u001b[39m         application=\u001b[38;5;28mself\u001b[39m.application,\n\u001b[32m   1598\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.login_timeout,\n\u001b[32m   1599\u001b[39m         backoff_generator=\u001b[38;5;28mself\u001b[39m._backoff_generator,\n\u001b[32m   1600\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1602\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauthenticate_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[38;5;28mself\u001b[39m._password = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n\u001b[32m   1605\u001b[39m \u001b[38;5;28mself\u001b[39m.auth_class.reset_secrets()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\connection.py:1938\u001b[39m, in \u001b[36mSnowflakeConnection.authenticate_with_retry\u001b[39m\u001b[34m(self, auth_instance)\u001b[39m\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauthenticate_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_instance) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1936\u001b[39m     \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_authenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1939\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   1940\u001b[39m         \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n\u001b[32m   1941\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mID token expired. Reauthenticating...: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, ex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\connection.py:1970\u001b[39m, in \u001b[36mSnowflakeConnection._authenticate\u001b[39m\u001b[34m(self, auth_instance)\u001b[39m\n\u001b[32m   1968\u001b[39m auth_instance._retry_ctx.set_start_time()\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1970\u001b[39m     \u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccount\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpasscode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_passcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpasscode_in_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_passcode_in_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmfa_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mfa_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_password_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1985\u001b[39m     logger.debug(\n\u001b[32m   1986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOperational Error raised at authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1987\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor authenticator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auth_instance).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1988\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\auth\\_auth.py:442\u001b[39m, in \u001b[36mAuth.authenticate\u001b[39m\u001b[34m(self, auth_instance, account, user, database, schema, warehouse, role, passcode, passcode_in_password, mfa_callback, password_callback, session_parameters, timeout)\u001b[39m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auth_instance, AuthByUsrPwdMfa):\n\u001b[32m    439\u001b[39m         \u001b[38;5;28mself\u001b[39m._delete_temporary_credential(\n\u001b[32m    440\u001b[39m             \u001b[38;5;28mself\u001b[39m._rest._host, user, TokenType.MFA_TOKEN\n\u001b[32m    441\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDatabaseError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmsg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFailed to connect to DB: \u001b[39;49m\u001b[38;5;132;43;01m{host}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[38;5;132;43;01m{port}\u001b[39;49;00m\u001b[33;43m. \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{message}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merrno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mER_FAILED_TO_CONNECT_TO_DB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msqlstate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    459\u001b[39m     logger.debug(\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtoken = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    461\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m         ),\n\u001b[32m    466\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\errors.py:298\u001b[39m, in \u001b[36mError.errorhandler_wrapper\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merrorhandler_wrapper\u001b[39m(\n\u001b[32m    277\u001b[39m     connection: SnowflakeConnection | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    281\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m    284\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \u001b[33;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     handed_over = \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[32m    305\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Error.errorhandler_make_exception(\n\u001b[32m    306\u001b[39m             error_class,\n\u001b[32m    307\u001b[39m             error_value,\n\u001b[32m    308\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\errors.py:361\u001b[39m, in \u001b[36mError.hand_to_other_handler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    363\u001b[39m         \u001b[38;5;66;03m# for async compatibility, check SNOW-1763096 and SNOW-1763103\u001b[39;00m\n\u001b[32m    364\u001b[39m         connection._errorhandler(connection, cursor, error_class, error_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\GitHub\\revenue_forecast\\.venv\\Lib\\site-packages\\snowflake\\connector\\errors.py:229\u001b[39m, in \u001b[36mError.default_errorhandler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    227\u001b[39m errno = error_value.get(\u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    228\u001b[39m done_format_msg = error_value.get(\u001b[33m\"\u001b[39m\u001b[33mdone_format_msg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[32m    230\u001b[39m     msg=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    231\u001b[39m     errno=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[32m    232\u001b[39m     sqlstate=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msqlstate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    233\u001b[39m     sfqid=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    234\u001b[39m     query=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    235\u001b[39m     done_format_msg=(\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[32m    237\u001b[39m     ),\n\u001b[32m    238\u001b[39m     connection=connection,\n\u001b[32m    239\u001b[39m     cursor=cursor,\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[31mDatabaseError\u001b[39m: 250001 (08001): Failed to connect to DB: cona-ccci.snowflakecomputing.com:443. The user you were trying to authenticate as differs from the user currently logged in at the IDP."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create session with external browser auth\n",
    "try:\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()\n",
    "    print(\"Using active Snowflake session\")\n",
    "except:\n",
    "    session = Session.builder.configs({\n",
    "        \"account\": \"cona-ccci\",\n",
    "        \"user\": \"wayne.jones@coca-cola.com\",\n",
    "        \"authenticator\": \"externalbrowser\",\n",
    "        \"role\": \"BI_P_ADMIN\",\n",
    "        \"warehouse\": \"BI_P_QRY_PU_WH\",\n",
    "        \"database\": \"DB_BI_P_SANDBOX\",\n",
    "        \"schema\": \"SANDBOX\"\n",
    "    }).create()\n",
    "    print(\"Created new Snowflake session with external browser auth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "parameters"
   },
   "outputs": [],
   "source": [
    "# === PARAMETERS ===\n",
    "RUN_ID = None  # set to a specific run_id string if you want; otherwise we'll auto-pick latest SUCCEEDED\n",
    "ASOF_SCOPE = \"GLOBAL__WAPE_MICRO_OVERALL\"\n",
    "OVERRIDE_SCOPE = \"PC_REASON__WAPE_MICRO_OVERALL__OVERRIDE5PCT\"\n",
    "EPS = 100.0\n",
    "MAX_HORIZON = 12\n",
    "EVAL_ANCHORS = 12               # last 12 anchors (months) to backtest\n",
    "OVERRIDE_MIN_REL_IMPROV = 0.05  # 5%\n",
    "MAPE_EPSILON = 100              # per your decision\n",
    "BIAS_MAX_ABS = 0.02             # 2% guardrail (tunable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "latest"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Snowflake notebook usually provides `session`\n",
    "if RUN_ID is None:\n",
    "    df = session.sql(\"\"\"\n",
    "      select run_id\n",
    "      from DB_BI_P_SANDBOX.SANDBOX.FORECAST_RUNS\n",
    "      where status = 'SUCCEEDED'\n",
    "      order by triggered_at desc\n",
    "      limit 1\n",
    "    \"\"\").to_pandas()\n",
    "    RUN_ID = df.iloc[0][\"RUN_ID\"]\n",
    "\n",
    "asof = session.sql(f\"\"\"\n",
    "  select asof_fiscal_yyyymm\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_RUNS\n",
    "  where run_id = '{RUN_ID}'\n",
    "\"\"\").to_pandas().iloc[0][\"ASOF_FISCAL_YYYYMM\"]\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"ASOF:\", asof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "experiment_row_register_models"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = \"EXP_GLOBAL_3A_V1\"\n",
    "now = datetime.utcnow()\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "merge into DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_EXPERIMENTS t\n",
    "using (select '{EXPERIMENT_ID}' experiment_id) s\n",
    "on t.experiment_id = s.experiment_id\n",
    "when not matched then insert (experiment_id, experiment_name, experiment_desc, created_by, created_at, tags)\n",
    "values (\n",
    "  '{EXPERIMENT_ID}',\n",
    "  'Global candidates + per-series overrides (3A)',\n",
    "  'Train global candidate models; choose global champion + per-series overrides by WAPE improvement.',\n",
    "  current_user(), current_timestamp(),\n",
    "  parse_json('{{\"type\":\"3A\",\"mape_epsilon\":{MAPE_EPSILON},\"override_min_rel_improv\":{OVERRIDE_MIN_REL_IMPROV}}}')\n",
    ");\n",
    "\"\"\").collect()\n",
    "\n",
    "def new_model_run_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# --- Model candidates (updated: removed Ridge) ---\n",
    "# See METRIC_AUDIT_REPORT.md for analysis - Ridge incompatible with signed_log1p transform\n",
    "CANDIDATES = [\n",
    "    # baseline: seasonal naive (computed in SQL; no sklearn needed)\n",
    "    {\"name\": \"SEASONAL_NAIVE_LAG12\", \"family\": \"baseline\",\n",
    "     \"params\": {\"kind\": \"seasonal_naive_lag12\"}},\n",
    "    \n",
    "    # ❌ REMOVED: Ridge regression - incompatible with exponential inverse transform\n",
    "    # Linear models extrapolate wildly in log-space, producing 1130% WAPE failure\n",
    "    # See METRIC_AUDIT_REPORT.md and docs/MODEL_TRANSFORM_GUIDE.md\n",
    "    \n",
    "    # gradient boosting (sklearn) - tree-based models are bounded, work well with log transforms\n",
    "    {\"name\": \"GBR_OHE\", \"family\": \"gbr\", \"params\": {\"target_transform\": \"signed_log1p\"}},\n",
    "]\n",
    "# --- end candidates ---\n",
    "\n",
    "model_runs = []\n",
    "for c in CANDIDATES:\n",
    "    mrid = new_model_run_id()\n",
    "    model_runs.append((c, mrid))\n",
    "    session.sql(f\"\"\"\n",
    "      insert into DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS\n",
    "      (model_run_id, run_id, asof_fiscal_yyyymm, experiment_id, model_scope, model_family, feature_set_id,\n",
    "       target_name, max_horizon, params, training_env, status, started_at, updated_at)\n",
    "      select\n",
    "        '{mrid}', '{RUN_ID}', {asof}, '{EXPERIMENT_ID}', 'GLOBAL',\n",
    "        '{c[\"family\"]}', 'OHE_V1',\n",
    "        'TOTAL_REVENUE', {MAX_HORIZON},\n",
    "        parse_json('{json.dumps(c[\"params\"])}'),\n",
    "        object_construct('python_version', '{sys.version}', 'sklearn_version', '{sklearn.__version__}'),\n",
    "        'PENDING', current_timestamp(), current_timestamp()\n",
    "    \"\"\").collect()\n",
    "\n",
    "print(f\"Registered {len(CANDIDATES)} model runs:\")\n",
    "for (c, mrid) in model_runs:\n",
    "    print(f\"  {c['name']:30s} → {mrid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d0c8-563c-43be-b8d0-e6dd930d10dd",
   "metadata": {
    "language": "python",
    "name": "load_snap_features"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "ds = session.sql(f\"\"\"\n",
    "  select *\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_DATASET_PC_REASON_H_SNAP\n",
    "  where run_id = '{RUN_ID}'\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# ---- Column map (case-safe) ----\n",
    "lower_cols = {c.lower(): c for c in ds.columns}\n",
    "\n",
    "# ---- Required ID columns (use actual column names from the table) ----\n",
    "required = [\n",
    "    \"roll_up_shop\",\n",
    "    \"reason_group\",\n",
    "    \"anchor_fiscal_yyyymm\",\n",
    "    \"anchor_month_seq\",\n",
    "    \"target_fiscal_yyyymm\",\n",
    "    \"target_month_seq\",\n",
    "    \"horizon\",\n",
    "    \"run_id\",\n",
    "]\n",
    "missing = [c for c in required if c not in lower_cols]\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset snap missing required columns: {missing}\")\n",
    "\n",
    "id_cols = [lower_cols[c] for c in required]\n",
    "\n",
    "# ---- Target column (explicit) ----\n",
    "y_col = \"Y_REVENUE\"\n",
    "if y_col not in ds.columns:\n",
    "    raise ValueError(f\"Target column '{y_col}' not found. Available columns: {ds.columns.tolist()}\")\n",
    "\n",
    "print(\"Using y_col:\", y_col)\n",
    "print(\"ID cols:\", id_cols)\n",
    "\n",
    "# ---- Feature selection ----\n",
    "# Exclude IDs + target. Also exclude bookkeeping columns you don't want as model inputs.\n",
    "exclude_cols = set(id_cols + [y_col, \"BUILT_AT\", \"ROW_HASH\"])\n",
    "\n",
    "# Optional: exclude BUDGET_TARGET from features to prevent \"cheating\" during backtest\n",
    "# if budget is not truly known at prediction time in your process. If budget is always known, remove this line.\n",
    "# exclude_cols.add(\"BUDGET_TARGET\")\n",
    "\n",
    "feature_cols = [c for c in ds.columns if c not in exclude_cols]\n",
    "\n",
    "# Split categorical vs numeric\n",
    "cat_cols = [c for c in feature_cols if ds[c].dtype == \"object\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "# Ensure series identifiers are included as categorical features for GLOBAL models\n",
    "for c in [\"ROLL_UP_SHOP\", \"REASON_GROUP\"]:\n",
    "    if c in ds.columns and c not in cat_cols:\n",
    "        cat_cols.append(c)\n",
    "        if c in num_cols:\n",
    "            num_cols.remove(c)\n",
    "\n",
    "print(\"Numeric features:\", len(num_cols))\n",
    "print(\"Categorical features:\", len(cat_cols))\n",
    "print(\"Example numeric cols:\", num_cols[:10])\n",
    "print(\"Example categorical cols:\", cat_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ca3b5-e662-4474-b25f-d497922ae7f4",
   "metadata": {
    "language": "python",
    "name": "add_horizon_feature"
   },
   "outputs": [],
   "source": [
    "# Add HORIZON as a numeric feature (we want one global model across horizons)\n",
    "if \"HORIZON\" not in num_cols:\n",
    "    num_cols.append(\"HORIZON\")\n",
    "\n",
    "# Fill nulls defensively\n",
    "ds[num_cols] = ds[num_cols].fillna(0)\n",
    "ds[cat_cols] = ds[cat_cols].fillna(\"UNKNOWN\")\n",
    "\n",
    "print(\"Numeric features (post):\", len(num_cols))\n",
    "print(\"Categorical features (post):\", len(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef96b19-f7ee-4f2c-81b9-a4ecdca1036e",
   "metadata": {
    "language": "python",
    "name": "log_helpers_model_factory"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def signed_log1p(x, eps: float):\n",
    "    \"\"\"\n",
    "    Signed log transform:\n",
    "      y = sign(x) * log1p(|x| / eps)\n",
    "\n",
    "    eps > 0 controls how aggressive the compression is.\n",
    "    \"\"\"\n",
    "    if eps is None or eps <= 0:\n",
    "        raise ValueError(\"eps must be > 0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.sign(x) * np.log1p(np.abs(x) / eps)\n",
    "\n",
    "def signed_expm1(y, eps: float):\n",
    "    \"\"\"\n",
    "    Inverse of signed_log1p:\n",
    "      x = sign(y) * eps * (expm1(|y|))\n",
    "    \"\"\"\n",
    "    if eps is None or eps <= 0:\n",
    "        raise ValueError(\"eps must be > 0\")\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return np.sign(y) * eps * np.expm1(np.abs(y))\n",
    "\n",
    "def signed_log10_1p(x, eps: float):\n",
    "    \"\"\"\n",
    "    Optional (not used yet): signed log10 variant.\n",
    "      y = sign(x) * log10(1 + |x|/eps)\n",
    "    \"\"\"\n",
    "    if eps is None or eps <= 0:\n",
    "        raise ValueError(\"eps must be > 0\")\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.sign(x) * np.log10(1.0 + (np.abs(x) / eps))\n",
    "\n",
    "def signed_pow10_m1(y, eps: float):\n",
    "    \"\"\"\n",
    "    Inverse of signed_log10_1p:\n",
    "      x = sign(y) * eps * ((10^|y|) - 1)\n",
    "    \"\"\"\n",
    "    if eps is None or eps <= 0:\n",
    "        raise ValueError(\"eps must be > 0\")\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return np.sign(y) * eps * (np.power(10.0, np.abs(y)) - 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdeaeec-ace1-426b-84a4-dad354473fc9",
   "metadata": {
    "language": "python",
    "name": "resolve_anchors_clear_prior_preds"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "anchor_seq_col = \"ANCHOR_MONTH_SEQ\"\n",
    "target_seq_col = \"TARGET_MONTH_SEQ\"\n",
    "pc_col = \"ROLL_UP_SHOP\"\n",
    "rg_col = \"REASON_GROUP\"\n",
    "h_col = \"HORIZON\"\n",
    "\n",
    "y_col = \"Y_REVENUE\"  # explicit\n",
    "\n",
    "anchors = sorted(ds[anchor_seq_col].unique())\n",
    "eval_anchors = anchors[-EVAL_ANCHORS:]  # last 12 anchors\n",
    "\n",
    "print(\"Anchors min/max:\", min(anchors), max(anchors))\n",
    "print(\"Eval anchors:\", eval_anchors)\n",
    "\n",
    "# Identify model_run_ids from earlier registration cell\n",
    "MODEL_RUN_IDS = [mrid for (_, mrid) in model_runs]\n",
    "print(\"MODEL_RUN_IDS:\", MODEL_RUN_IDS)\n",
    "\n",
    "# Clear prior predictions for these model_run_ids (rerunnable)\n",
    "mrid_list_sql = \", \".join([f\"'{x}'\" for x in MODEL_RUN_IDS])\n",
    "session.sql(f\"\"\"\n",
    "delete from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id in ({mrid_list_sql});\n",
    "\"\"\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbde6a-3662-4ada-8cdb-77a93aad5ed8",
   "metadata": {
    "language": "python",
    "name": "backtest_RIDGE_GBR"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pred_rows = []\n",
    "now = datetime.utcnow()\n",
    "\n",
    "# ---- Ensure eval_anchors are plain ints (avoid np.int8 etc) ----\n",
    "eval_anchors = [int(x) for x in eval_anchors]\n",
    "\n",
    "# ---- Ensure model_runs exists (don’t rely on hidden state) ----\n",
    "# Expected upstream vars (depending on your notebook):\n",
    "# - candidates: list[dict] with {\"name\": ...}\n",
    "# - model_run_ids: dict mapping name -> uuid\n",
    "if \"model_runs\" not in globals() or model_runs is None:\n",
    "    if \"candidates\" in globals() and \"model_run_ids\" in globals():\n",
    "        model_runs = [(c, model_run_ids[c[\"name\"]]) for c in candidates if c.get(\"name\") in model_run_ids]\n",
    "    elif \"CANDIDATES\" in globals() and \"model_run_ids\" in globals():\n",
    "        model_runs = [(c, model_run_ids[c[\"name\"]]) for c in CANDIDATES if c.get(\"name\") in model_run_ids]\n",
    "    else:\n",
    "        raise NameError(\n",
    "            \"model_runs is not defined and could not be inferred. \"\n",
    "            \"Define `model_runs = [(candidate_dict, model_run_id), ...]` in experiment_row_register_models.\"\n",
    "        )\n",
    "\n",
    "# Also keep a convenience list for later leaderboard cells\n",
    "MODEL_RUN_IDS = [mrid for _, mrid in model_runs]\n",
    "\n",
    "# Optional: only used if you want to prevent huge Ridge blowups in transformed space.\n",
    "RIDGE_SLOG_CLIP_MARGIN = 0.25\n",
    "\n",
    "for (cand, mrid) in model_runs:\n",
    "    cname = cand[\"name\"]\n",
    "    if cname == \"SEASONAL_NAIVE_LAG12\":\n",
    "        continue  # baseline handled in SQL\n",
    "\n",
    "    pipe = make_model(cname)\n",
    "\n",
    "    for a in eval_anchors:\n",
    "        # Train on rows whose targets would have been known at anchor a:\n",
    "        train = ds[(ds[target_seq_col] <= a)].copy()\n",
    "        test  = ds[(ds[anchor_seq_col] == a)].copy()\n",
    "\n",
    "        # drop null targets defensively\n",
    "        train = train[train[y_col].notna()]\n",
    "        test  = test[test[y_col].notna()]\n",
    "\n",
    "        if train.empty or test.empty:\n",
    "            continue\n",
    "\n",
    "        X_train = train[num_cols + cat_cols]\n",
    "        X_test  = test[num_cols + cat_cols]\n",
    "\n",
    "        y_train = train[y_col].astype(float).to_numpy()\n",
    "\n",
    "        # ---- Transform with explicit EPS from parameters ----\n",
    "        y_train_t = signed_log1p(y_train, eps=EPS)\n",
    "\n",
    "        # Optional guard: clip for Ridge only (helps prevent extreme extrapolation)\n",
    "        if cname.upper().startswith(\"RIDGE\"):\n",
    "            lo = np.nanmin(y_train_t) - RIDGE_SLOG_CLIP_MARGIN\n",
    "            hi = np.nanmax(y_train_t) + RIDGE_SLOG_CLIP_MARGIN\n",
    "\n",
    "        pipe.fit(X_train, y_train_t)\n",
    "        yhat_t = pipe.predict(X_test)\n",
    "\n",
    "        if cname.upper().startswith(\"RIDGE\"):\n",
    "            yhat_t = np.clip(yhat_t, lo, hi)\n",
    "\n",
    "        yhat = signed_expm1(yhat_t, eps=EPS)\n",
    "\n",
    "        # Append row-wise predictions for Snowflake table\n",
    "        for j, (_, r) in enumerate(test.iterrows()):\n",
    "            pred_rows.append({\n",
    "                \"MODEL_RUN_ID\": mrid,\n",
    "                \"ROLL_UP_SHOP\": str(r[pc_col]),\n",
    "                \"REASON_GROUP\": str(r[rg_col]),\n",
    "                \"ANCHOR_FISCAL_YYYYMM\": int(r[\"ANCHOR_FISCAL_YYYYMM\"]),\n",
    "                \"ANCHOR_MONTH_SEQ\": int(r[anchor_seq_col]),\n",
    "                \"HORIZON\": int(r[h_col]),\n",
    "                \"TARGET_FISCAL_YYYYMM\": int(r[\"TARGET_FISCAL_YYYYMM\"]),\n",
    "                \"TARGET_MONTH_SEQ\": int(r[target_seq_col]),\n",
    "                \"Y_TRUE\": float(r[y_col]),\n",
    "                \"Y_PRED\": float(yhat[j]),\n",
    "                \"Y_PRED_LO\": None,\n",
    "                \"Y_PRED_HI\": None,\n",
    "                \"CREATED_AT\": now,\n",
    "                \"DETAILS\": {\"eps\": float(EPS), \"candidate\": cname, \"eval_anchor\": int(a)}\n",
    "            })\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows)\n",
    "print(\"Python prediction rows:\", len(pred_df))\n",
    "pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeb222-90a2-4334-9371-2bfdc883b41f",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "print(\"EPS:\", EPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b788b-a0cf-4c92-8d6c-560f6b85caf2",
   "metadata": {
    "language": "python",
    "name": "write_to_sql"
   },
   "outputs": [],
   "source": [
    "if len(pred_df) > 0:\n",
    "    session.write_pandas(\n",
    "        pred_df,\n",
    "        table_name=\"FORECAST_MODEL_BACKTEST_PREDICTIONS\",\n",
    "        database=\"DB_BI_P_SANDBOX\",\n",
    "        schema=\"SANDBOX\",\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d20d4-caf0-4298-9347-45451a900e04",
   "metadata": {
    "language": "python",
    "name": "get_columns"
   },
   "outputs": [],
   "source": [
    "# Detect column names in FORECAST_ACTUALS_PC_REASON_MTH_SNAP (no guessing)\n",
    "cols = session.sql(\"\"\"\n",
    "select column_name\n",
    "from DB_BI_P_SANDBOX.INFORMATION_SCHEMA.COLUMNS\n",
    "where table_schema = 'SANDBOX'\n",
    "  and table_name   = 'FORECAST_ACTUALS_PC_REASON_MTH_SNAP'\n",
    "order by ordinal_position\n",
    "\"\"\").to_pandas()[\"COLUMN_NAME\"].tolist()\n",
    "\n",
    "print(\"ACTUALS_SNAP columns:\", cols)\n",
    "\n",
    "# Find the month sequence column\n",
    "if \"MONTH_SEQ\" in cols:\n",
    "    actuals_seq_col = \"MONTH_SEQ\"\n",
    "else:\n",
    "    raise ValueError(\"Could not find MONTH_SEQ in FORECAST_ACTUALS_PC_REASON_MTH_SNAP. Paste the column list above.\")\n",
    "\n",
    "# Find the revenue/actual value column (ordered candidates)\n",
    "actual_candidates = [\n",
    "    \"REVENUE\", \"ACTUAL_REVENUE\", \"Y_REVENUE\", \"TOTAL_REVENUE\",\n",
    "    \"REVENUE_MTH\", \"ACTUALS_REVENUE\", \"ACTUAL\"\n",
    "]\n",
    "actuals_y_col = next((c for c in actual_candidates if c in cols), None)\n",
    "\n",
    "if actuals_y_col is None:\n",
    "    raise ValueError(\n",
    "        \"Could not find an actuals revenue column in FORECAST_ACTUALS_PC_REASON_MTH_SNAP. \"\n",
    "        f\"Columns are: {cols}\"\n",
    "    )\n",
    "\n",
    "print(\"Using actuals_seq_col:\", actuals_seq_col)\n",
    "print(\"Using actuals_y_col:\", actuals_y_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e1b15-52b0-4712-a7cd-046b7ff187a1",
   "metadata": {
    "language": "python",
    "name": "baseline_seasonal_naive"
   },
   "outputs": [],
   "source": [
    "baseline_mrid = [mrid for (c, mrid) in model_runs if c[\"name\"] == \"SEASONAL_NAIVE_LAG12\"][0]\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "insert into DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "(model_run_id, roll_up_shop, reason_group,\n",
    " anchor_fiscal_yyyymm, anchor_month_seq, horizon,\n",
    " target_fiscal_yyyymm, target_month_seq,\n",
    " y_true, y_pred, y_pred_lo, y_pred_hi,\n",
    " created_at, details)\n",
    "with ds as (\n",
    "  select *\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_DATASET_PC_REASON_H_SNAP\n",
    "  where run_id = '{RUN_ID}'\n",
    "),\n",
    "eval_anchors as (\n",
    "  select distinct ds.anchor_month_seq as anchor_month_seq\n",
    "  from ds\n",
    "  qualify dense_rank() over (order by ds.anchor_month_seq desc) <= {EVAL_ANCHORS}\n",
    "),\n",
    "base as (\n",
    "  select\n",
    "    ds.roll_up_shop,\n",
    "    ds.reason_group,\n",
    "    ds.anchor_fiscal_yyyymm,\n",
    "    ds.anchor_month_seq,\n",
    "    ds.horizon,\n",
    "    ds.target_fiscal_yyyymm,\n",
    "    ds.target_month_seq,\n",
    "    ds.y_revenue::number as y_true\n",
    "  from ds\n",
    "  join eval_anchors ea\n",
    "    on ea.anchor_month_seq = ds.anchor_month_seq\n",
    "),\n",
    "lag12 as (\n",
    "  select\n",
    "    a.roll_up_shop,\n",
    "    a.reason_group,\n",
    "    a.{actuals_seq_col} as month_seq,\n",
    "    a.{actuals_y_col}   as y_lag12\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_ACTUALS_PC_REASON_MTH_SNAP a\n",
    "  where a.run_id = '{RUN_ID}'\n",
    ")\n",
    "select\n",
    "  '{baseline_mrid}',\n",
    "  b.roll_up_shop, b.reason_group,\n",
    "  b.anchor_fiscal_yyyymm, b.anchor_month_seq, b.horizon,\n",
    "  b.target_fiscal_yyyymm, b.target_month_seq,\n",
    "  b.y_true,\n",
    "  l.y_lag12 as y_pred,\n",
    "  null, null,\n",
    "  current_timestamp(),\n",
    "  parse_json('{{\"baseline\":\"seasonal_naive_lag12\"}}')\n",
    "from base b\n",
    "left join lag12 l\n",
    "  on l.roll_up_shop = b.roll_up_shop\n",
    " and l.reason_group = b.reason_group\n",
    " and l.month_seq = (b.target_month_seq - 12);\n",
    "\"\"\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a725d6b-a6ac-4bcb-8a6b-86b738921cbe",
   "metadata": {
    "language": "python",
    "name": "validate_prediction_coverage"
   },
   "outputs": [],
   "source": [
    "# Show how many backtest predictions we have per model_run_id\n",
    "mrid_list_sql = \", \".join([f\"'{x}'\" for x in MODEL_RUN_IDS])\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "select\n",
    "  model_run_id,\n",
    "  count(*) as rows_pred,\n",
    "  count_if(y_pred is null) as null_preds,\n",
    "  count_if(y_true is null) as null_true,\n",
    "  min(anchor_month_seq) as min_anchor_seq,\n",
    "  max(anchor_month_seq) as max_anchor_seq,\n",
    "  min(horizon) as min_h,\n",
    "  max(horizon) as max_h\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id in ({mrid_list_sql})\n",
    "group by 1\n",
    "order by 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102f555-6b4c-406d-8095-2967844a103d",
   "metadata": {
    "language": "python",
    "name": "horizon_coverage"
   },
   "outputs": [],
   "source": [
    "# Horizon coverage by anchor for ONE model_run_id (pick any; they should match)\n",
    "one_mrid = MODEL_RUN_IDS[0]\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "select\n",
    "  anchor_month_seq,\n",
    "  count(*) as rowz,\n",
    "  count(distinct horizon) as horizons_present,\n",
    "  min(horizon) as min_h,\n",
    "  max(horizon) as max_h,\n",
    "  min(target_month_seq) as min_target_seq,\n",
    "  max(target_month_seq) as max_target_seq\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id = '{one_mrid}'\n",
    "group by 1\n",
    "order by 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec5246-b359-4b20-868f-bc7d9730e922",
   "metadata": {
    "language": "python",
    "name": "fixed_grid"
   },
   "outputs": [],
   "source": [
    "one_mrid = session.sql(f\"\"\"\n",
    "  select model_run_id\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS\n",
    "  where run_id = '{RUN_ID}'\n",
    "  order by started_at desc\n",
    "  limit 1\n",
    "\"\"\").to_pandas().iloc[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f780cda-762f-41e7-8afb-51e5051fdea6",
   "metadata": {
    "language": "python",
    "name": "compute_metrics"
   },
   "outputs": [],
   "source": [
    "# --- Inputs ---\n",
    "fixed_eval_anchors = [48]\n",
    "anchor_list_sql = \", \".join([str(x) for x in fixed_eval_anchors])\n",
    "anchor_array_sql = \"array_construct(\" + \", \".join([str(x) for x in fixed_eval_anchors]) + \")\"\n",
    "\n",
    "mrid_list_sql = \", \".join([f\"'{x}'\" for x in MODEL_RUN_IDS])\n",
    "\n",
    "baseline_mrid = [mrid for (c, mrid) in model_runs if c[\"name\"] == \"SEASONAL_NAIVE_LAG12\"][0]\n",
    "\n",
    "# --- Clear existing metrics for reruns ---\n",
    "session.sql(f\"\"\"\n",
    "delete from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_METRICS\n",
    "where model_run_id in ({mrid_list_sql})\n",
    "\"\"\").collect()\n",
    "\n",
    "# --- Compute + insert metrics (note: INSERT ... WITH ... SELECT ...) ---\n",
    "session.sql(f\"\"\"\n",
    "insert into DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_METRICS\n",
    "(model_run_id, metric_scope, metric_name, horizon, value, computed_at, details)\n",
    "\n",
    "with p as (\n",
    "  select *\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "  where model_run_id in ({mrid_list_sql})\n",
    "    and anchor_month_seq in ({anchor_list_sql})\n",
    "),\n",
    "naive as (\n",
    "  select\n",
    "    roll_up_shop,\n",
    "    reason_group,\n",
    "    anchor_month_seq,\n",
    "    horizon,\n",
    "    abs(y_true - y_pred) as abs_naive_err\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "  where model_run_id = '{baseline_mrid}'\n",
    "    and anchor_month_seq in ({anchor_list_sql})\n",
    "),\n",
    "p3 as (\n",
    "  select\n",
    "    p.*,\n",
    "    (p.y_true - p.y_pred) as err,\n",
    "    (p.y_pred - p.y_true) as err_signed,\n",
    "    abs(p.y_true - p.y_pred) as abs_err,\n",
    "    abs(p.y_true) as abs_y,\n",
    "    n.abs_naive_err\n",
    "  from p\n",
    "  left join naive n\n",
    "    on n.roll_up_shop = p.roll_up_shop\n",
    "   and n.reason_group = p.reason_group\n",
    "   and n.anchor_month_seq = p.anchor_month_seq\n",
    "   and n.horizon = p.horizon\n",
    "),\n",
    "\n",
    "-- MICRO\n",
    "micro_overall as (\n",
    "  select\n",
    "    model_run_id,\n",
    "    'OVERALL' as metric_scope,\n",
    "    null::number as horizon,\n",
    "    avg(abs_err) as mae,\n",
    "    sqrt(avg(err*err)) as rmse,\n",
    "    sum(abs_err) / nullif(sum(abs_y),0) as wape,\n",
    "    avg(iff(abs_y >= {MAPE_EPSILON}, abs_err/abs_y, null)) as mape_eps,\n",
    "    avg(abs_err) / nullif(avg(abs_naive_err),0) as mase,\n",
    "    sum(err_signed) / nullif(sum(abs_y),0) as bias\n",
    "  from p3\n",
    "  group by 1\n",
    "),\n",
    "micro_by_h as (\n",
    "  select\n",
    "    model_run_id,\n",
    "    'BY_HORIZON' as metric_scope,\n",
    "    horizon,\n",
    "    avg(abs_err) as mae,\n",
    "    sqrt(avg(err*err)) as rmse,\n",
    "    sum(abs_err) / nullif(sum(abs_y),0) as wape,\n",
    "    avg(iff(abs_y >= {MAPE_EPSILON}, abs_err/abs_y, null)) as mape_eps,\n",
    "    avg(abs_err) / nullif(avg(abs_naive_err),0) as mase,\n",
    "    sum(err_signed) / nullif(sum(abs_y),0) as bias\n",
    "  from p3\n",
    "  group by 1,3\n",
    "),\n",
    "\n",
    "-- MACRO (per series, then average)\n",
    "series_overall as (\n",
    "  select\n",
    "    model_run_id,\n",
    "    roll_up_shop,\n",
    "    reason_group,\n",
    "    'OVERALL' as metric_scope,\n",
    "    null::number as horizon,\n",
    "    avg(abs_err) as mae,\n",
    "    sqrt(avg(err*err)) as rmse,\n",
    "    sum(abs_err) / nullif(sum(abs_y),0) as wape,\n",
    "    avg(iff(abs_y >= {MAPE_EPSILON}, abs_err/abs_y, null)) as mape_eps,\n",
    "    avg(abs_err) / nullif(avg(abs_naive_err),0) as mase,\n",
    "    sum(err_signed) / nullif(sum(abs_y),0) as bias\n",
    "  from p3\n",
    "  group by 1,2,3\n",
    "),\n",
    "series_by_h as (\n",
    "  select\n",
    "    model_run_id,\n",
    "    roll_up_shop,\n",
    "    reason_group,\n",
    "    'BY_HORIZON' as metric_scope,\n",
    "    horizon,\n",
    "    avg(abs_err) as mae,\n",
    "    sqrt(avg(err*err)) as rmse,\n",
    "    sum(abs_err) / nullif(sum(abs_y),0) as wape,\n",
    "    avg(iff(abs_y >= {MAPE_EPSILON}, abs_err/abs_y, null)) as mape_eps,\n",
    "    avg(abs_err) / nullif(avg(abs_naive_err),0) as mase,\n",
    "    sum(err_signed) / nullif(sum(abs_y),0) as bias\n",
    "  from p3\n",
    "  group by 1,2,3,5\n",
    "),\n",
    "macro_agg as (\n",
    "  select\n",
    "    model_run_id,\n",
    "    metric_scope,\n",
    "    horizon,\n",
    "    avg(mae) as mae,\n",
    "    avg(rmse) as rmse,\n",
    "    avg(wape) as wape,\n",
    "    avg(mape_eps) as mape_eps,\n",
    "    avg(mase) as mase,\n",
    "    avg(bias) as bias\n",
    "  from (\n",
    "    select model_run_id, metric_scope, horizon, mae, rmse, wape, mape_eps, mase, bias from series_overall\n",
    "    union all\n",
    "    select model_run_id, metric_scope, horizon, mae, rmse, wape, mape_eps, mase, bias from series_by_h\n",
    "  )\n",
    "  group by 1,2,3\n",
    ")\n",
    "\n",
    "-- Emit rows into FORECAST_MODEL_METRICS\n",
    "select\n",
    "  model_run_id,\n",
    "  metric_scope,\n",
    "  metric_name,\n",
    "  horizon,\n",
    "  value,\n",
    "  current_timestamp(),\n",
    "  object_construct('series_agg','MICRO','eval_anchors',{anchor_array_sql},'mape_epsilon',{MAPE_EPSILON})\n",
    "from (\n",
    "  select model_run_id, metric_scope, horizon, 'MAE' as metric_name, mae as value from micro_overall\n",
    "  union all select model_run_id, metric_scope, horizon, 'RMSE', rmse from micro_overall\n",
    "  union all select model_run_id, metric_scope, horizon, 'WAPE', wape from micro_overall\n",
    "  union all select model_run_id, metric_scope, horizon, 'MAPE_EPS', mape_eps from micro_overall\n",
    "  union all select model_run_id, metric_scope, horizon, 'MASE', mase from micro_overall\n",
    "  union all select model_run_id, metric_scope, horizon, 'BIAS', bias from micro_overall\n",
    "\n",
    "  union all select model_run_id, metric_scope, horizon, 'MAE', mae from micro_by_h\n",
    "  union all select model_run_id, metric_scope, horizon, 'RMSE', rmse from micro_by_h\n",
    "  union all select model_run_id, metric_scope, horizon, 'WAPE', wape from micro_by_h\n",
    "  union all select model_run_id, metric_scope, horizon, 'MAPE_EPS', mape_eps from micro_by_h\n",
    "  union all select model_run_id, metric_scope, horizon, 'MASE', mase from micro_by_h\n",
    "  union all select model_run_id, metric_scope, horizon, 'BIAS', bias from micro_by_h\n",
    ")\n",
    "\n",
    "union all\n",
    "\n",
    "select\n",
    "  model_run_id,\n",
    "  metric_scope,\n",
    "  metric_name,\n",
    "  horizon,\n",
    "  value,\n",
    "  current_timestamp(),\n",
    "  object_construct('series_agg','MACRO','eval_anchors',{anchor_array_sql},'mape_epsilon',{MAPE_EPSILON})\n",
    "from (\n",
    "  select model_run_id, metric_scope, horizon, 'MAE' as metric_name, mae as value from macro_agg\n",
    "  union all select model_run_id, metric_scope, horizon, 'RMSE', rmse from macro_agg\n",
    "  union all select model_run_id, metric_scope, horizon, 'WAPE', wape from macro_agg\n",
    "  union all select model_run_id, metric_scope, horizon, 'MAPE_EPS', mape_eps from macro_agg\n",
    "  union all select model_run_id, metric_scope, horizon, 'MASE', mase from macro_agg\n",
    "  union all select model_run_id, metric_scope, horizon, 'BIAS', bias from macro_agg\n",
    ")\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Cell 11 done. Anchors:\", fixed_eval_anchors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccca2d9-28e3-4056-85be-eb37fb631dd8",
   "metadata": {
    "language": "python",
    "name": "leaderboard"
   },
   "outputs": [],
   "source": [
    "# --- leaderboard (replace entire cell) ---\n",
    "mrid_list_sql = \", \".join([f\"'{x}'\" for x in MODEL_RUN_IDS])\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "with s as (\n",
    "  select\n",
    "    m.model_run_id,\n",
    "    r.params:\"candidate\"::string as candidate,\n",
    "    m.value as wape_micro_overall,\n",
    "    r.updated_at\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_METRICS m\n",
    "  join DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS r\n",
    "    on r.model_run_id = m.model_run_id\n",
    "  where m.model_run_id in ({mrid_list_sql})\n",
    "    and m.metric_scope = 'OVERALL'\n",
    "    and m.metric_name  = 'WAPE'\n",
    "    and m.details:\"series_agg\"::string = 'MICRO'\n",
    "),\n",
    "best as (\n",
    "  select *\n",
    "  from s\n",
    "  qualify row_number() over (\n",
    "    partition by candidate\n",
    "    order by wape_micro_overall asc, updated_at desc\n",
    "  ) = 1\n",
    ")\n",
    "select model_run_id, candidate, wape_micro_overall\n",
    "from best\n",
    "order by wape_micro_overall\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765635c-f297-47f8-b86a-ba72a545d361",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "select\n",
    "  params:\"candidate\"::string as candidate,\n",
    "  model_run_id,\n",
    "  created_at\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS\n",
    "where run_id = '6673357e-8195-4146-9e36-17209b6cca57'\n",
    "  and params:\"candidate\"::string = 'RIDGE_OHE'\n",
    "order by created_at;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d07a9-dc56-409c-9247-a0a0355e2bb5",
   "metadata": {
    "language": "python",
    "name": "persist_champion"
   },
   "outputs": [],
   "source": [
    "# --- Inputs you already have ---\n",
    "# RUN_ID, MODEL_RUN_IDS, MAPE_EPSILON, fixed_eval_anchors\n",
    "fixed_eval_anchors = [48]\n",
    "anchor_array_sql = \"array_construct(\" + \", \".join([str(x) for x in fixed_eval_anchors]) + \")\"\n",
    "\n",
    "# 1) Get ASOF_FISCAL_YYYYMM for this RUN_ID\n",
    "asof = session.sql(f\"\"\"\n",
    "select asof_fiscal_yyyymm\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_RUNS\n",
    "where run_id = '{RUN_ID}'\n",
    "\"\"\").to_pandas().iloc[0][\"ASOF_FISCAL_YYYYMM\"]\n",
    "\n",
    "# 2) Pick champion by WAPE / MICRO / OVERALL (fixed-grid anchors already embedded in metrics.details)\n",
    "champ = session.sql(f\"\"\"\n",
    "with scores as (\n",
    "  select\n",
    "    m.model_run_id,\n",
    "    r.params:\"candidate\"::string as candidate,\n",
    "    m.value as wape_micro_overall\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_METRICS m\n",
    "  join DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS r\n",
    "    on r.model_run_id = m.model_run_id\n",
    "  where r.run_id = '{RUN_ID}'\n",
    "    and m.metric_scope = 'OVERALL'\n",
    "    and m.metric_name = 'WAPE'\n",
    "    and m.details:\"series_agg\"::string = 'MICRO'\n",
    "  order by wape_micro_overall\n",
    ")\n",
    "select * from scores limit 1\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "champ_mrid = champ.iloc[0][\"MODEL_RUN_ID\"]\n",
    "champ_candidate = champ.iloc[0][\"CANDIDATE\"]\n",
    "champ_wape = float(champ.iloc[0][\"WAPE_MICRO_OVERALL\"])\n",
    "\n",
    "print(\"Champion:\", champ_candidate, champ_mrid, \"WAPE=\", champ_wape, \"ASOF=\", asof)\n",
    "\n",
    "# 3) Upsert into FORECAST_MODEL_CHAMPIONS using sentinel keys for GLOBAL scope\n",
    "session.sql(f\"\"\"\n",
    "merge into DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_CHAMPIONS t\n",
    "using (\n",
    "  select\n",
    "    {asof}::number as asof_fiscal_yyyymm,\n",
    "    'GLOBAL'::string as champion_scope,\n",
    "    '__ALL__'::string as roll_up_shop,\n",
    "    '__ALL__'::string as reason_group,\n",
    "    '{champ_mrid}'::string as model_run_id,\n",
    "    'WAPE_MICRO_OVERALL'::string as selection_metric,\n",
    "    object_construct(\n",
    "      'series_agg','MICRO',\n",
    "      'metric_scope','OVERALL',\n",
    "      'metric','WAPE',\n",
    "      'wape', {champ_wape},\n",
    "      'eval_anchors', {anchor_array_sql},\n",
    "      'mape_epsilon', {MAPE_EPSILON},\n",
    "      'run_id', '{RUN_ID}',\n",
    "      'candidate', '{champ_candidate}'\n",
    "    ) as selection_logic,\n",
    "    current_timestamp() as selected_at,\n",
    "    current_user() as selected_by\n",
    ") s\n",
    "on  t.asof_fiscal_yyyymm = s.asof_fiscal_yyyymm\n",
    "and t.champion_scope     = s.champion_scope\n",
    "and t.roll_up_shop       = s.roll_up_shop\n",
    "and t.reason_group       = s.reason_group\n",
    "when matched then update set\n",
    "  model_run_id      = s.model_run_id,\n",
    "  selection_metric  = s.selection_metric,\n",
    "  selection_logic   = s.selection_logic,\n",
    "  selected_at       = s.selected_at,\n",
    "  selected_by       = s.selected_by\n",
    "when not matched then insert (\n",
    "  asof_fiscal_yyyymm, champion_scope, roll_up_shop, reason_group,\n",
    "  model_run_id, selection_metric, selection_logic, selected_at, selected_by\n",
    ") values (\n",
    "  s.asof_fiscal_yyyymm, s.champion_scope, s.roll_up_shop, s.reason_group,\n",
    "  s.model_run_id, s.selection_metric, s.selection_logic, s.selected_at, s.selected_by\n",
    ")\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Champion upserted into FORECAST_MODEL_CHAMPIONS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde8d42-0645-477a-a68c-ce50bc1b8aa5",
   "metadata": {
    "language": "python",
    "name": "whats_wrong_ridge_1"
   },
   "outputs": [],
   "source": [
    "mrid_list_sql = \", \".join([f\"'{x}'\" for x in MODEL_RUN_IDS])\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "select\n",
    "  r.params:\"candidate\"::string as candidate,\n",
    "  p.model_run_id,\n",
    "  count(*) as n,\n",
    "  min(p.y_true) as y_true_min,\n",
    "  avg(p.y_true) as y_true_avg,\n",
    "  max(p.y_true) as y_true_max,\n",
    "  min(p.y_pred) as y_pred_min,\n",
    "  avg(p.y_pred) as y_pred_avg,\n",
    "  max(p.y_pred) as y_pred_max,\n",
    "  sum(abs(p.y_true - p.y_pred)) / nullif(sum(abs(p.y_true)),0) as wape\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS p\n",
    "join DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS r\n",
    "  on r.model_run_id = p.model_run_id\n",
    "where p.model_run_id in ({mrid_list_sql})\n",
    "  and p.anchor_month_seq in (48)\n",
    "group by 1,2\n",
    "order by wape\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfdc00-f32e-4b1a-838f-095138dd4135",
   "metadata": {
    "language": "python",
    "name": "whats_wrong_ridge_2"
   },
   "outputs": [],
   "source": [
    "ridge_mrid = session.sql(f\"\"\"\n",
    "select model_run_id\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_RUNS\n",
    "where run_id = '{RUN_ID}'\n",
    "  and params:\"candidate\"::string = 'RIDGE_OHE'\n",
    "limit 1\n",
    "\"\"\").to_pandas().iloc[0][\"MODEL_RUN_ID\"]\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "select\n",
    "  roll_up_shop,\n",
    "  reason_group,\n",
    "  anchor_fiscal_yyyymm,\n",
    "  horizon,\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  (y_true - y_pred) as err,\n",
    "  abs(y_true - y_pred) as abs_err\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id = '{ridge_mrid}'\n",
    "  and anchor_month_seq in (48)\n",
    "order by abs_err desc\n",
    "limit 30\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926fa5e-a4af-42cf-b18f-22fa6d20ef32",
   "metadata": {
    "language": "python",
    "name": "whats_wrong_ridge_3"
   },
   "outputs": [],
   "source": [
    "EPS = 100  # your epsilon\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "with p as (\n",
    "  select y_true, y_pred\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "  where model_run_id = '{ridge_mrid}'\n",
    "    and anchor_month_seq in (48)\n",
    "),\n",
    "t as (\n",
    "  select\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    -- signed log transform of y_true\n",
    "    case\n",
    "      when y_true >= 0 then ln(1 + abs(y_true)/{EPS})\n",
    "      else -ln(1 + abs(y_true)/{EPS})\n",
    "    end as y_true_slog\n",
    "  from p\n",
    ")\n",
    "select\n",
    "  avg(abs(y_pred - y_true)) as avg_abs_err_on_raw,\n",
    "  avg(abs(y_pred - y_true_slog)) as avg_abs_err_on_slog,\n",
    "  corr(y_pred, y_true) as corr_raw,\n",
    "  corr(y_pred, y_true_slog) as corr_slog\n",
    "from t\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334bfc3-17af-4031-87f6-7621d143f93b",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "ridge_mrid = \"6673357e-8195-4146-9e36-17209b6cca57\"  # from 13A\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "select\n",
    "  count(*) as n,\n",
    "  min(y_true) as y_true_min,\n",
    "  avg(y_true) as y_true_avg,\n",
    "  max(y_true) as y_true_max,\n",
    "  min(y_pred) as y_pred_min,\n",
    "  avg(y_pred) as y_pred_avg,\n",
    "  max(y_pred) as y_pred_max\n",
    "from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id = '{ridge_mrid}'\n",
    "  and anchor_month_seq = 48\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1ae06-268b-4585-a5c5-3a67d4d0f9b5",
   "metadata": {
    "language": "python",
    "name": "patch_ridge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_mrid = \"6673357e-8195-4146-9e36-17209b6cca57\"\n",
    "EVAL_ANCHORS = [48]\n",
    "TRAIN_MAX_ANCHOR = min(EVAL_ANCHORS) - 1\n",
    "\n",
    "# Load dataset snap for this RUN_ID\n",
    "ds = session.sql(f\"\"\"\n",
    "  select *\n",
    "  from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_DATASET_PC_REASON_H_SNAP\n",
    "  where run_id = '{RUN_ID}'\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Target + IDs\n",
    "y_col = \"Y_REVENUE\"\n",
    "id_cols = [\"RUN_ID\",\"ROLL_UP_SHOP\",\"REASON_GROUP\",\"ANCHOR_FISCAL_YYYYMM\",\"ANCHOR_MONTH_SEQ\",\n",
    "           \"TARGET_FISCAL_YYYYMM\",\"TARGET_MONTH_SEQ\",\"HORIZON\"]\n",
    "\n",
    "# Base feature set (start from your earlier computed cols, but re-derive safely)\n",
    "base_exclude = set(id_cols + [y_col])\n",
    "\n",
    "feature_cols = [c for c in ds.columns if c not in base_exclude]\n",
    "\n",
    "# Split numeric vs categorical\n",
    "cat_cols = [c for c in feature_cols if ds[c].dtype == \"object\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "# Ensure series identifiers are categorical features\n",
    "for c in [\"ROLL_UP_SHOP\",\"REASON_GROUP\"]:\n",
    "    if c not in cat_cols:\n",
    "        cat_cols.append(c)\n",
    "\n",
    "# Drop ID-like / key-like numeric columns that break linear models\n",
    "DROP_NUM = set([\n",
    "    \"ASOF_FISCAL_YYYYMM\",\n",
    "    \"ANCHOR_FISCAL_YYYYMM\",\n",
    "    \"TARGET_FISCAL_YYYYMM\",\n",
    "    \"ANCHOR_MONTH_SEQ\",\n",
    "    \"TARGET_MONTH_SEQ\",\n",
    "    \"ANCHOR_FISCAL_YEAR\",\n",
    "    \"ANCHOR_FISCAL_MONTH\",\n",
    "])\n",
    "num_cols = [c for c in num_cols if c not in DROP_NUM]\n",
    "\n",
    "print(\"Ridge numeric cols:\", len(num_cols))\n",
    "print(\"Ridge categorical cols:\", len(cat_cols))\n",
    "\n",
    "# Train/test split by anchor\n",
    "train = ds[ds[\"ANCHOR_MONTH_SEQ\"] <= TRAIN_MAX_ANCHOR].copy()\n",
    "test  = ds[ds[\"ANCHOR_MONTH_SEQ\"].isin(EVAL_ANCHORS)].copy()\n",
    "\n",
    "X_train = train[num_cols + cat_cols]\n",
    "y_train = train[y_col].astype(float)\n",
    "\n",
    "X_test  = test[num_cols + cat_cols]\n",
    "y_test  = test[y_col].astype(float)\n",
    "\n",
    "# Pipeline: impute + scale numeric, OHE categorical\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=0)\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Build prediction rows (match table columns dynamically)\n",
    "pred = test[id_cols].copy()\n",
    "pred[\"MODEL_RUN_ID\"] = ridge_mrid\n",
    "pred[\"Y_TRUE\"] = y_test.values\n",
    "pred[\"Y_PRED\"] = y_pred\n",
    "pred[\"BUILT_AT\"] = np.datetime64(\"now\")\n",
    "\n",
    "# Align to actual prediction table columns\n",
    "pred_cols = session.sql(\"\"\"\n",
    "select column_name\n",
    "from DB_BI_P_SANDBOX.INFORMATION_SCHEMA.COLUMNS\n",
    "where table_schema='SANDBOX'\n",
    "  and table_name='FORECAST_MODEL_BACKTEST_PREDICTIONS'\n",
    "order by ordinal_position\n",
    "\"\"\").to_pandas()[\"COLUMN_NAME\"].tolist()\n",
    "pred_cols_set = set(pred_cols)\n",
    "\n",
    "# keep only columns that exist in the table (case-sensitive to Snowflake output)\n",
    "# our pandas cols are uppercase already; if not, upper them\n",
    "pred.columns = [c.upper() for c in pred.columns]\n",
    "pred = pred[[c for c in pred.columns if c in pred_cols_set]]\n",
    "\n",
    "# Overwrite Ridge rows for these anchors (rerunnable)\n",
    "session.sql(f\"\"\"\n",
    "delete from DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\n",
    "where model_run_id = '{ridge_mrid}'\n",
    "  and anchor_month_seq in ({\",\".join([str(x) for x in EVAL_ANCHORS])})\n",
    "\"\"\").collect()\n",
    "\n",
    "session.create_dataframe(pred).write.mode(\"append\").save_as_table(\n",
    "    \"DB_BI_P_SANDBOX.SANDBOX.FORECAST_MODEL_BACKTEST_PREDICTIONS\"\n",
    ")\n",
    "\n",
    "print(\"Ridge predictions overwritten for anchors:\", EVAL_ANCHORS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "lastEditStatus": {
   "authorEmail": "nbalje@ccbcc.com",
   "authorId": "3300832511315",
   "authorName": "NBALJE",
   "lastEditTime": 1769777279630,
   "notebookId": "yhx54rbpsuqcaueaon3g",
   "sessionId": "794ae108-f49e-4d07-a8c4-02a90b022d05"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
